##### Kubernetes cluster install manual for CentOS 7 minimal

### OS setup START ###

# Set SELinux in permissive mode (effectively disabling it). This is required to allow containers to access the host filesystem, which is needed by pod networks for example. You have to do this until SELinux support is improved in the kubelet.
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# turn off firewall to avoid firewall-related problems.. just for sure :)
systemctl disable firewalld && systemctl stop firewalld

# register repository with install binaries for kubernetes for yum
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

# in case of not resolving packages for kubernetes fix dns
cat <<EOF > /etc/resolv.conf 
nameserver 8.8.8.8
nameserver 8.8.4.4
EOF

# install theses repos (kubernetes-cni is Kubernetes Container Networking Interface, it can be configured using WeaveNet)
yum install -y nano docker kubelet kubeadm kubectl kubernetes-cni

# enable and start docker
systemctl enable docker && systemctl start docker

# enable and start kubelet (kubelet is the primary “node agent” that runs on each node. It can register the node with the apiserver)
systemctl enable kubelet && systemctl start kubelet

# some users on RHEL/CentOS 7 have reported issues with traffic being routed incorrectly due to iptables being bypassed. You should ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config, e.g.
sysctl -w net.bridge.bridge-nf-call-iptables=1
echo "net.bridge.bridge-nf-call-iptables=1" > /etc/sysctl.d/k8s.conf
echo "net.bridge.bridge-nf-call-ip6tables=1" > /etc/sysctl.d/k8s.conf

# kubelete wont run if swap is enabled. Following command just comment the swap using 'sed'
swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab

### OS setup END - you can start cloning ###
### set up hostnames and configure DNS ###

# set hostname to for clones to node1.k8s, resp. node2.k8s
hostnamectl --static set-hostname node1.k8s

# edit dns
nano /etc/hosts

# insert IP obtained from 'ip addr show' from each node, i.e..
192.168.1.100 master.k8s
192.168.0.101 node1.k8s
192.168.0.102 node2.k8s

### set up master node, master.k8s
# master node requires 2 CPU

# set up the cluster
kubeadm init

# you will receive something like following
# kubeadm join 192.168.0.119:6443 --token hmgme6.plr8hl1h3oru98mu --discovery-token-ca-cert-hash sha256:589eecb20f70bdb038e5ab4c9e6785ae4c54e8c3be90d04d44e4b0efe758d9fd

# configure kubectl to connect to created cluster by setting env var to config to created cluster, otherwise its gonna try to connect localhost:8080 
#for nodes, it is /etc/kubernetes/kubelet.conf
export KUBECONFIG=/etc/kubernetes/admin.conf

# check configuration by getting the pods
kubectl get pods -n kube-system

### register worker nodes using kubectl join from command got by kubeadm init..

# after joining the workers, they are still 'NotReady', until you config Container Network Plugin. Let's use WeaveNet (you can debug this problem using i.e. 'kubectl describe node node1.k8s', that says 'NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized'). This will create DaemonSet and few security-related resources, which make the nodes ready
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

# your cluster is ready, it should look like this
[root@master ~]# kubectl get node
NAME         STATUS   ROLES    AGE     VERSION
master.k8s   Ready    master   3h29m   v1.17.0
node1.k8s    Ready    <none>   176m    v1.17.0
node2.k8s    Ready    <none>   3m3s    v1.17.0

[root@master ~]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-67ljq             1/1     Running   0          95m   10.40.0.1       master.k8s   <none>           <none>
kube-system   coredns-6955765f44-s6rqp             1/1     Running   0          95m   10.40.0.2       master.k8s   <none>           <none>
kube-system   etcd-master.k8s                      1/1     Running   4          95m   192.168.1.100   master.k8s   <none>           <none>
kube-system   kube-apiserver-master.k8s            1/1     Running   4          95m   192.168.1.100   master.k8s   <none>           <none>
kube-system   kube-controller-manager-master.k8s   1/1     Running   4          95m   192.168.1.100   master.k8s   <none>           <none>
kube-system   kube-proxy-2qmng                     1/1     Running   0          89m   192.168.1.102   node2.k8s    <none>           <none>
kube-system   kube-proxy-fvdw8                     1/1     Running   4          95m   192.168.1.100   master.k8s   <none>           <none>
kube-system   kube-proxy-tct6z                     1/1     Running   0          90m   192.168.1.101   node1.k8s    <none>           <none>
kube-system   kube-scheduler-master.k8s            1/1     Running   4          95m   192.168.1.100   master.k8s   <none>           <none>
kube-system   weave-net-b894g                      2/2     Running   0          11m   192.168.1.102   node2.k8s    <none>           <none>
kube-system   weave-net-dskd7                      2/2     Running   0          11m   192.168.1.100   master.k8s   <none>           <none>
kube-system   weave-net-tdvrr                      2/2     Running   0          11m   192.168.1.101   node1.k8s    <none>           <none>

# ensure static IP by editing network interface
# file /etc/sysconfig/network-scripts/ifcfg-enp0s3
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="static" #changed from 'dhcp'
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="enp0s3"
UUID="de2f6afb-9f96-4929-8d5c-a3d70bff82ee"
DEVICE="enp0s3"
ONBOOT="yes"
IPADDR="192.168.1.100" #added
GATEWAY="192.168.1.1" #added
NETMASK="255.255.255.0" #added

# if you are behind proxy, you also may want to set it for docker
# file /etc/systemd/system/docker.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=5.9.73.93:8080/"
Environment="NO_PROXY=localhost,127.0.0.1" #here may also go your local image repositories..
Environment="HTTPS_PROXY=85.222.191.222:36337/"

# create simple NodeJs app
# create Dockerfile

# you should add your user to docker group or work under root
usermod -a -G docker david

# build image from Dockerfile
docker build -t davidsapp .

# try out some deployments, services, whatever..

# ingress
# set up all needed resources with given yamls
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml

# for bare metal, set up a nodeport to access the nodes
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml

# istio
# download 
curl -L https://istio.io/downloadIstio | sh -

# register in PATH
export PATH=$PWD/bin:$PATH

# go in demo mode (contains also monitoring provided by prometheus, grafana)
istioctl manifest apply --set profile=demo
